# sol

---

### 一、选择题（每题 3 分）

1. **答案：C**
    * **解析：** 该优化目标是带 $L_2$ 正则化的线性回归（岭回归）。增加 $\lambda$ 会增大正则化强度，降低模型的复杂度。这通常会导致模型在训练集上的拟合能力下降（**偏差增加**），但模型的泛化能力提高，预测结果的波动变小（**方差减少**）。

2. **答案：B**
    * **解析：** 在软间隔 SVM 中，松弛因子 $\xi$ 的含义如下：
        * $\xi = 0$：样本分类正确且在间隔边界外或边界上。
        * $0 < \xi < 1$：样本分类正确，但落在了间隔边界与决策边界之间。
        * $\xi = 1$：样本恰好在决策边界上。
        * $\xi > 1$：样本被错误分类。
        图中的点 2 位于正类间隔线和决策线之间，属于分类正确但进入了间隔带的情况，因此 $0 < \xi_2 < 1$。

3. **答案：D**
    * **解析：** 对于多分类任务，输出层通常需要输出各类别的概率分布，且概率之和为 1，因此标准做法是使用 **softmax** 激活函数。

4. **答案：B**
    * **解析：** 朴素贝叶斯（尤其是多项式朴素贝叶斯）通常用于处理离散/类别特征。虽然高斯朴素贝叶斯可以处理连续值，但在通用模型对比中，它最常被归类为处理离散特征。逻辑回归、决策树和线性模型都有成熟的机制直接处理连续数值。

5. **答案：B**
    * **解析：** 集成学习的核心原则是“好而不同”。
        * A：基模型可以来自不同算法（如 Stacking）。
        * B：**正确**，基模型相关性越低，集成后的错误抵消效果越好。
        * C：基模型数量达到一定程度后效果会趋于平稳，甚至可能因过度拟合或噪声积累而变差。
        * D：随机森林的差异来自样本采样（Bagging）和**特征采样**。

6. **答案：D**
    * **解析：** 随机森林属于 Bagging 框架，各决策树之间是并行的、独立的，没有强依赖关系。强依赖关系是 Boosting（如 AdaBoost, GBDT）的特点。

7. **答案：D**
    * **解析：** 距离度量的性质包括：非负性、同一性、对称性、直递性（三角不等式）。**归一性**不是距离度量的必要性质。

8. **答案：C**
    * **解析：** PCA 的目标是找到方差最大的投影方向以保留最多的信息。因此，应该选择方差**最大**的主成分，而不是最小的。

9. **答案：A**
    * **解析：** 无监督学习没有标签信息，仅通过数据的特征来挖掘内在结构（如聚类、降维）。B 错误（降维属于无监督）；C 错误（SVM 是有监督算法）。

10. **答案：B**
    * **解析：** 利用 **d-分离（d-separation）** 判定：
        * B 选项中，给定 $X_6$。由于 $X_2 \rightarrow X_6 \leftarrow X_5$ 是对撞结构（Collider），观测到 $X_6$ 会激活该路径，使得 $X_2$ 和 $X_5$ **不独立**。

---

### 三、简答题

**11. 正则化对训练误差的影响**

* **分析：** 该数据集是线性可分的。如果 $C$ 取得无限大，对应的正则项 $w_k^2$ 必须趋于 0。
  * **罚 $w_1$ 或 $w_2$：** 意味着模型被迫变为水平或垂直的决策边界。由于样本是沿对角线分布的，强制去掉 $x_1$ 或 $x_2$ 维度的权重会导致模型无法完美分割数据，**训练误差会变大**。
  * **罚 $w_0$：** 意味着决策边界必须经过原点。如果数据分布的中心不在原点，强制 $w_0=0$ 也会导致无法完美分割，**训练误差会变大**。

**12. (2) 深度学习易受对抗攻击的原因**

* **解释：** 在高维空间中，输入的每一个维度加一个微小扰动 $\delta$，其对输出的影响是累积的。输出的变化量 $\Delta y \approx \sum w_i \delta_i$。即使每个 $\delta_i$ 极小，但在数千甚至数万个维度下，如果扰动方向与权重的符号一致，这些微小变化的累加足以让最终的输出结果产生巨大的偏移，从而导致误分类。

**12. (1) 评价指标公式**

* $Precision = \frac{TP}{TP + FP}$
* $Recall = \frac{TP}{TP + FN}$
* $F1 = \frac{2 \times Precision \times Recall}{Precision + Recall}$

**12. (2) macro-F1 与指标选择**

* **定义：** $macro\text{-}F1 = \frac{1}{C} \sum_{c=1}^C F1_c$，即先算每一类的 F1 再求平均。
* **分析：** 当类别不平衡时，**macro-F1** 更科学。因为 micro-F1 会被样本量大的类别主导，可能掩盖模型在小类别上的极差表现；而 macro-F1 平等对待每一类，能更好地反映模型对少数类的识别能力。

**13. PCA 的两个优化目标角度**

* **角度 1（最大方差）：** 寻找一个投影子空间，使得数据在该空间上的投影方差最大化，即尽可能保留原始数据的分布信息。
* **角度 2（最小重构代价）：** 寻找一个投影子空间，使得原始数据点与其在子空间上的投影点之间的平方距离之和（重构误差）最小。

---

### 四、计算题

**14. 蘑菇分类决策树**

* **(1) 熵：** 样本 A-H 中，Poisonous 为 0 的有 3 个（A,B,C），为 1 的有 5 个（D,E,F,G,H）。
    $H(S) = -(\frac{3}{8}\log_2\frac{3}{8} + \frac{5}{8}\log_2\frac{5}{8}) \approx 0.954$。
* **(2) 根节点：** 计算各特征的信息增益。
  * 以 $IsHeavy$ 为例：$IsHeavy=1$ 时（C,D,H），标签为 $\{0, 1, 1\}$；$IsHeavy=0$ 时，标签为 $\{0, 0, 1, 1, 1\}$。计算得出增益，通常选取增益最大的。观察发现 $IsSmooth$ 和 $IsHeavy$ 的分类效果较好。

**15. 朴素贝叶斯分类**

* **(1) 预测：** 使用拉普拉斯平滑（$k=2$ 类别，$n_i=3$ 特征取值）。
  * $P(Buy) = 7/12, P(No) = 5/12$。
  * 计算条件概率：$P(少年|Buy) = (1+1)/(6+3) = 2/9$, $P(中等|Buy) = (3+1)/(6+3) = 4/9$, $P(高|Buy) = (1+1)/(6+3) = 2/9$。
  * $S(Buy) = \frac{7}{12} \times \frac{2}{9} \times \frac{4}{9} \times \frac{2}{9} \approx 0.0128$。
  * 同理计算 $S(No)$，比较大小得出结果。通常此例中预测为“买”。
* **(2) 挑战：** 1. 正态分布假设不成立（数据非正态）；2. 异常值对均值和方差的影响巨大。

**16. K-means 聚类**

* **第一次迭代：**
  * 计算各点到 $C_1(2,1), C_2(1,3)$ 的距离。
  * 分配结果：$G_1 = \{(3,2), (4,1), (2,0)\}, G_2 = \{(1,6), (0,2), (0,3)\}$。
  * 更新中心：$C_1' = (\frac{3+4+2}{3}, \frac{2+1+0}{3}) = (3, 1)$；$C_2' = (\frac{1+0+0}{3}, \frac{6+2+3}{3}) \approx (0.33, 3.67)$。

**17. AdaBoost 强分类器**

* **步骤：**
    1. 初始化权重 $w_i = 1/10$。
    2. 寻找最优阈值 $v$ 使误差率 $\epsilon$ 最小。观察 $y$ 的变化，在 $x=4$ 与 $x=5$ 之间，或 $x=9$ 与 $x=10$ 之间可能存在切分点。
    3. 计算 $\alpha = \frac{1}{2}\ln\frac{1-\epsilon}{\epsilon}$，更新权重，迭代产生强分类器 $H(x) = \text{sign}(\sum \alpha_t h_t(x))$。
